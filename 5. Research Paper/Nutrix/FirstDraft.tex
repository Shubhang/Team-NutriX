

%\documentclass[conference]{IEEEtran}
\documentclass[twocolumn]{article}
\usepackage{graphicx}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}




\if CLASSINFOpdf

\else




% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{NuTriX: A smart way to recommend your Foods}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Dr. Mahfuza Farooque}
%\IEEEauthorblockA{School of Computer Engineering and Electrical \\\\
%Pennsylvania State University\\
%University Park, PA 16802\\
%Email: http://www.michaelshell.org/contact.html}
\and
\IEEEauthorblockN{Dr. Suman Saha}
%\IEEEauthorblockA{School of Computer Engineering and Electrical \\\\
%Pennsylvania State University\\
%University Park, PA 16802\\
%Email: http://www.michaelshell.org/contact.html}%\and
\and
\IEEEauthorblockN{Shubhang Sharma}
%\IEEEauthorblockA{School of Computer Engineering and Electrical \\\\
%Pennsylvania State University\\
%University Park, PA 16802\\
%Email: http://www.michaelshell.org/contact.html}%\and
\and
\IEEEauthorblockN{Phakphum Artkaew}
%\IEEEauthorblockA{School of Computer Engineering and Electrical \\\\
%Pennsylvania State University\\
%University Park, PA 16802\\
%Email: http://www.michaelshell.org/contact.html}%\and
\and
\IEEEauthorblockN{Josephine Prickett}
%\IEEEauthorblockA{School of Computer Engineering and Electrical \\\\
%	Pennsylvania State University\\
%	University Park, PA 16802\\
%	Email: http://www.michaelshell.org/contact.html}%\and
}




% make the title area
\maketitle

\begin{abstract}
This project searches for a manner in which to predict the dietary restrictions within a food based off an ingredient list. Intuitively, some dietary restrictions are identifiable without question. For example, cheese will trigger a flag for lactose intolerance. The additional step that this project takes is identifying the probability of other dietary restrictions being triggered as a result of correlation between ingredients. To extend the aforementioned example, our machine might detect that a recipe with cheese might also have high sodium content, triggering high blood pressure as a dietary restriction. 
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




\section{Introduction}
% no \IEEEPARstart
Food plays an important role in our every life; it is essential for the human body to function. People have different preferable food depending on various conditions such as allergens, or congenital disease. Therefore, the food recommendation system is one of the important research areas. There is adequate research related to food recommendation and machine learning, as there is more public data available than before. 
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

The recent research on food recommendations is various. Mostly, researchers were able to predict food’s ingredients by taking a picture. They used various machine learning techniques such as SVM, multiclass- Adaboost, etc. One notable work is, “Food-101 – Mining Discriminative Components with Random Forests”, which relies on Random Forests Discriminative Components to classify foods into 101 categories. Researchers put emphasis on their use of super pixels, comparable to mosaic tiles, as a stand in for image patches, as they claim are best for object detection.The RFDC model performed well on their own testing data at 50.76\% accuracy, only being outperformed by Convolution Neural Network by about 6\%. However, when tested on MIT-Indoor data at 58.36\%, it was outperformed by Discriminative Mode Seeking and Intuitionistic Fuzzy Values by over 2-8\%.With even more fruitful results, “  Automatic Chinese Food Identification and Quantity Estimation” relies heavily on sparse coding and SVM to classify foods into 50 categories. Researchers did not compare their models to others developed in the field, but rather compared different variations of their models against each other. They found that Multi-class Adaboost with Top-5 accuracy proved to be the most accurate, with an accuracy of 90.9\%. However, the average accuracy of their work is 68.3\%.

In this paper, we target to develop a system that can classify food by its ingredients from pictures and texts. We aim to create two machine learning models; first is picture to text model, second is text to text model. Each model will be tested on a different machine learning model and compared to see which one performs better. The classes of model within this paper include model that involve clustering by vectors, statistical methods, and gradient boosting algorithms. 


\begin{figure*}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{"System Architecture"}
	\caption{Nutrix System Arcitechture}
	\label{fig:system-architecture}
\end{figure*}


\section{Datasets}

Our study relies on the data retrieved from an online machine learning platform, kaggle.com. The website contains free datasets published by its users, mainly used for data science purposes. We three datasets; the first two were found on kaggle.com, the third dataset was found on eightportions.com. The first dataset, ingredient v1.csv was uploaded by the company Datafiniti under the CC BY-NC-SA 4.0 copyright. Datafiniti compiled 10,000 entries containing attributes such as “brand, name, manufacturer, category, features, and more” (features being the ingredients list). The second dataset was created by Elisa for the purpose of creating a food recommendation system. The dataset was collected from allrecipes.com; it contains about 50,000 recipes, including its name, ingredients, image\_url, cooking\_direction, nutrition, and id. The third dataset was created by Ryan Lee under the ODC Attribution license. Lee scraped this dataset from various websites; it contains about 125,000 recipes, including name, ingredients, instructions, source, and picture. 

In essence, the required data needed in each dataset was an ingredients list for each food entry, which were provided in both of the files collected from Kaggle. As aforementioned, our first dataset contained 10,000 entries and the second one contained 50,000. With that being said, a considerable amount of the entries contained null values for certain values, which was the first obstacle encountered during the project. An even bigger issue that was encountered was that a considerable amount of entries were non-food items (such as Clorox), causing noise in the data.  

\section{Preprocessing}
The preprocessing of data proved to be the most extensive part of the process; the steps taken have been broken down into four parts below:
\subsection{Cleaning Data}
As a result of the data being sourced through opportunistic methods rather than experimentally, missing and inappropriate data proved to be a cumbersome obstacle. For ingredients v1.csv, there were some ingredients that contain special characters such as \^i; some of the food was left as none. It might affect the accuracy of the model so first, we removed special characters and none from the dataset. For core-data\_recipe.csv, we neglected some columns because the dataset was too big. The columns that were removed were image\_url, cooking ingredients, and nutrition.
\subsection{Classifying Dietary Restrictions & Feature Selections}
 We created a dictionary of allergens that indicate each dietary restriction and ingredients that contain them. We aimed to perform a machine learning model on following nine different dietary restrictions; lactose intolerance, celiac, diabetes, high blood pressure, allergens, halal, kosher, vegetarian, and vegan. Then, we performed a string check on each dataset to detect allergens. For example, when we looked at the ingredients of ‘Potato Bacon Pizza.’, we detected that there was cheese. Thus, we added lactose intolerance into its allergens column. Referencing back to the issue of non-food data entries mentioned in “Overview of Datasets”, one more round of data clearing was applied after classifying dietary restrictions. As a fix to the issue, data entries that contained no allergens were removed, since non-food items would not raise a flag on any of the dietary restrictions. 
\subsection{Feature Scaling}
\subsection{One Hot Encoding}
Since the ingredients column contained entries of lists. The data was therefore three dimensional. To lower the array's dimensionality back down to two, one hot encoding was performed. As a result, each possible ingredient was then listed as its own column and each row displayed a tally of how many times said ingredient shows up in the recipe.
\subsection{Separating into Feature Vectors and Target Variables}
After One Hot Encoding the ingredients, one more step remained before feeding our data into the models - the separation of variables. The target variable was an array of the different dietary restrictions that was looped through, allowing us to apply the model on each one. 
\section{Training and Testing Data}
One of the challenges faced when searching for correlations of each individual dietary restriction was that the models only identified the correlations for one target variable at a time. However, the process of training and testing the model based off of each restriction followed identical form without loss of generality. Therefore, a for each loop was applied on the set of dietary restrictions, calculating the correlations for each dietary restriction one at a time. The models applied and their hyper parameters are detailed below.

\subsection{Linear Support Vector Classifier (SVC)}
The Linear SVC model operates by separating mapping data into a hyperplane in which data points can be separated by straight lines. In this research, sklearn's LinearSVC was applied with the default parameters listed within the documentation. The average test accuracy over the ten food restriction classes was 92.41%.
\subsection{Naive Bayes Algorithm}
The Naive Bayes Algorithm is rooted in the usage of Bayes' Theorem from statistics. The aim of the theorem is to find the probability than event A will occur on the condition that B has also occurred. In this work, the theorem is used to calculate how probable it is that a dietary restriction will occur given the fact that other restrictions were also flagged. The default settings for the GaussianNB() function from sklearn's naive bayes package was used.
\subsection{Catboost}
Catboost is a gradient boost algorithm that is primarily used for categorical variables. One hallmark of Catboost is its generation of one-hot encoded variables without the need for external encoding. For the sake of this work, since the all categories were already pre-encoded, the pre-existing one hot encoded categories were fed into this machine. The results of this model averaged at 99.17, proving to be the highest scoring model studied. 
\subsection{Adaboost (Adaptive Boosting)}
In the Adaboost algorithm, focus is aimed towards improving on the gradient boosting algorithm itself by assigning weights to each classifier based on its tested accuracy. In this work, the Adaboost algorithm was used with 100 estimators and a random state of zero. The model took the longest to run, yet yielded the lowest results - averaging at 45.65 percent. Additionally, the results lacked precison, facing lows of 4.6 percent accuracy for Kosher and maxing at 93.6 percent for Halah. 
\subsection{Test Results}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{"NEW-data_recipe-none-removed results"}
	\caption{Test Results on NEW-data_recipe-none-removed.csv}
	\label{fig:new-datarecipe-none-removed-results}
\end{figure}


\section{Postprocessing and \\Visualization}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{"Test Accuracy on _NEW-data_recipe-none-removed.csv_"}
	\caption{Test Results on Each Diet Restriction}
	\label{fig:test-accuracy-on-new-datarecipe-none-removed}
\end{figure}
After each model was run, the testing accuracy was recorded into Google Sheets. The averages for each model throughout each food group was calculated and the results were plotted.


\section{Results}
As seen in the above figures, Catboost proved to perform superior to all other models on the dataset. Not only did it score an average test result that was higher than all other, it scored higher accuracy than any other model for each dietary restriction as well. As mentioned Section 4.3, Catboost is design specifically for categorical data. Hence it's performance.

On the other side of the spectrum, Adaboost yielded the lowest results. Not only did the results prove to be inaccurate, they also proved to be extremely unprecise, opposite of Catboost.

These results spark much interest, as both Catboost and Adaboost share many similarities, both being gradient boosting algorithms, yet seemed to have polar opposite reactions to this dataset. 
\section{Conclusion}
In conclusion, further research could include the fine tuning of hyper parameters within the models being used. 




% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}




% that's all folks
\end{document}


